# MoE + RL on GSM8K (Single-GPU)

Fine-tune an **off-the-shelf Mixture of Experts (MoE)** model with **policy-gradient RL (PPO-style)** on **GSM8K**.

- Default model: `microsoft/Phi-tiny-MoE-instruct` (≈3.8B total / 1.1B activated).  
- Single-GPU friendly via **LoRA** + optional **4-bit** loading.  
- Clean modular code for: model / data / RL / training / evaluation.  

---

## 🚀 Quickstart

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Training
python -m src.train --config configs/default.yaml

# Evaluation
python -m src.evaluate --config configs/default.yaml
```

Push to GitHub:

```bash
git init && git add . && git commit -m "Initial commit"
git branch -M main
git remote add origin https://github.com/<you>/<repo>.git
git push -u origin main
```

---

## 📦 Requirements

Install the core dependencies:

```bash
pip install flash_attn
pip install transformers accelerate peft datasets
```

---

## 🧩 Model Setup

This project uses a **Mixture of Experts (MoE)** built on top of a transformer backbone.  
The pre-trained model is 👉 [huihui-ai/Huihui-MoE-1B-A0.6B-SFT](https://huggingface.co/huihui-ai/Huihui-MoE-1B-A0.6B-SFT).  

**Selection criteria:**
- Lightweight transformer (~1B parameters) with MoE, feasible on a single GPU.  
- Router mechanism selects **top-k experts per token**, balancing efficiency and specialization.  
- Widely available on Hugging Face, ensuring reproducibility and extension.  

### Sparse MoE
- **Gate:** `Linear(1024 → 3)` → logits over 3 experts; top-k (usually k=1 or 2) chosen per token.  
- **Experts:** 3 × `Qwen3MoeMLP`, each with `gate_proj`, `up_proj`, and `down_proj`.  

### LoRA
Low-Rank Adaptation with rank **r=16**.  
Trainable parameters are ~1% of the original base model.  

### Value Head
A small regressor that predicts the scalar value (expected reward) of a generated sequence.  
- Structure: `Linear(1024 → 1024) → Tanh → Linear(1024 → 1)`  
- Serves as a **critic** in an Actor–Critic setup.  

---

## 📊 GSM8K Dataset

**GSM8K (Grade School Math 8K)** is a benchmark dataset with ~8,500 grade-school-level math word problems.  

- **Train set:** ~7,500 problems  
- **Test set:** ~1,300 problems  

---

## ⚙️ Training at a Glance

- **Micro-batch size:** 4 prompts (limited by GPU memory).  
- **Gradient accumulation:** 32 micro-steps → effective batch ≈ 128 prompts per optimizer update.  
- **Optimizer updates:** 1200 (≈20 epochs over GSM8K).  
- **Learning rates:**  
  - LoRA parameters: `1e-5`  
  - Value head: `5e-5`  

This setup balances efficiency with stability, while ensuring the model sees the entire dataset multiple times.  

---

## 🎯 Reinforcement Learning

### Core RL Components
- **State:** The prompt (math problem).  
- **Action:** The sequence of tokens generated by the model (answer).  
- **Policy:** The language model with LoRA adapters, mapping prompts → token distributions.  
- **Reward:** Based on correctness of the final numeric answer:  
  - `1.0` if correct, `0.0` if wrong.  
  - +0.05 bonus if explicit final-answer marker is used (`Final Answer`, `####`, `\boxed`).  
  - -0.05 penalty for excessively long outputs.  
- **Advantage:** `(reward − value prediction)` from the Value Head, normalized for stability.  

### Additional Components
- **Value Head (Critic):** Learns to approximate expected reward, making the setup Actor–Critic.  
- **KL Penalty (Regularizer):** Keeps the fine-tuned policy close to the frozen reference model, preventing reward hacking or drift.  
- **Entropy Bonus:** Encourages exploration and diversity in completions.  
- **Group Baseline:** Reduces variance by comparing completions of the same prompt.  

---
### Loss Functions
- **Reward centered**: This is the raw reward for K=1, or reward minus group mean if K>1.
- **Value Prediction Loss:** The value head predicts the return for each prompt/output, with the target set to rewards_centered (the raw reward for K=1; ow. reward - group mean). 
- **Policy Gradien Loss:** The policy increases or decreases token probabilities in proportion to the advantage (rewards_centered − value prediction)
- **KL Divergence Loss**: A regularizer that penalizes divergence from the frozen reference model, keeping updates stable and preventing drift (across batches).

---
  
## 📌 Notes
- The value prediction loss updates the value head but policy gradient loss updates the policy.
- We do not use the raw reward to update the policy because it can be unstable moving from one batch to another.
- KL divergence is a standard measure of how one probability distribution differs from another: as the extra information needed when one distribution is used to approximate the other.
- Default configuration is in `configs/default.yaml`.  
- Outputs and checkpoints are saved under `runs/`.  
- Training is GPU-intensive — adjust batch size and accumulation for your hardware.  

---
